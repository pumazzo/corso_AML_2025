# -*- coding: utf-8 -*-
"""Coloring_potts_hamiltonian.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/19X_QWbW3sUrasG-r2B4lyvNGMAzS8dgf
"""

! wget  https://mat.tepper.cmu.edu/COLOR/instances/queen5_5.col
! wget https://mat.tepper.cmu.edu/COLOR/instances/queen13_13.col

!pip install torch-geometric

import torch
import torch.nn as nn
import torch.nn.functional as F
from torch_geometric.data import Data
from torch_geometric.nn import GCNConv
import networkx as nx
import matplotlib.pyplot as plt
from torch.optim.lr_scheduler import LambdaLR

# ---------- Utility Functions ----------
def load_col_file(filename):
    edges = []
    nodes = set()
    with open(filename, 'r') as f:
        for line in f:
            if line.startswith('e'):
                _, u, v = line.strip().split()
                u, v = int(u) - 1, int(v) - 1
                edges.append((u, v))
                nodes.update([u, v])
    edge_index = torch.tensor(edges, dtype=torch.long).t().contiguous()
    num_nodes = len(nodes)
    return edge_index, num_nodes

def visualize_graph(edge_index, num_nodes):
    G = nx.Graph()
    G.add_edges_from(edge_index.t().tolist())
    plt.figure(figsize=(6, 6))
    nx.draw(G, with_labels=True, node_color='lightblue', edge_color='gray')
    plt.title("Graph Visualization")
    plt.show()

def plot_node_predictions(edge_index, probs, title="Predicted Clusters"):
    G = nx.Graph()
    G.add_edges_from(edge_index.t().tolist())
    preds = probs.argmax(dim=1).cpu().numpy()
    num_classes = probs.shape[1]
    cmap = plt.get_cmap("tab10", num_classes)
    fig, ax = plt.subplots(figsize=(6, 6))
    pos = nx.spring_layout(G, seed=42)
    nx.draw_networkx_nodes(G, pos, node_color=preds, cmap=cmap, ax=ax, node_size=600)
    nx.draw_networkx_edges(G, pos, ax=ax, edge_color="gray")
    nx.draw_networkx_labels(G, pos, ax=ax)
    sm = plt.cm.ScalarMappable(cmap=cmap)
    sm.set_array([])
    cbar = plt.colorbar(sm, ax=ax, ticks=range(num_classes))
    cbar.set_label("Predicted Class")
    ax.set_title(title)
    ax.axis("off")
    plt.tight_layout()
    plt.show()

# ---------- Loss Functions ----------
def loss_func_pyg(probs, edge_index):
    src, dst = edge_index
    sim = (probs[src] * probs[dst]).mean(dim=1) # 2*H/n_connection
    return sim.sum() / 2

def loss_func_color_hard_pyg(preds, edge_index):
    src, dst = edge_index
    conflict = (preds[src] == preds[dst]) & (src != dst)
    return conflict.sum()

# ---------- Learning Rate Scheduler ----------
def cyclical_temperature_schedule(base_lr, warmup_epochs=10, cold_factor=0.1,hot_factor=0.1, cycle_length=100,hot_phase_length=5):
    def schedule(epoch):
        if epoch < warmup_epochs:
          return (epoch + 1) / warmup_epochs
        #hot_phase_length = hot_phase_length#int(cycle_length * 0.1)
        phase = epoch % cycle_length
        return hot_factor if phase < hot_phase_length else cold_factor
    return schedule

# ---------- GNN Model ----------
class SimpleGCN(nn.Module):
    def __init__(self, num_nodes, dim_embedding, hidden_channels, num_classes, dropout=0.2, noise_std=0.01):
        super().__init__()
        self.embed = nn.Embedding(num_nodes, dim_embedding)
        self.conv1 = GCNConv(dim_embedding, hidden_channels)
        self.conv2 = GCNConv(hidden_channels, num_classes)
        self.dropout = nn.Dropout(p=dropout)
        self.noise_std = noise_std

    def forward(self, data):
        x = self.embed(torch.arange(data.num_nodes, device=data.edge_index.device))
        if self.training and self.noise_std > 0:
            x = x + torch.randn_like(x) * self.noise_std

        x = self.conv1(x, data.edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, data.edge_index)
        return F.softmax(x, dim=1)

from torch_geometric.nn import SAGEConv

class SimpleSAGE(nn.Module):
    def __init__(self, num_nodes, dim_embedding, hidden_channels, num_classes, dropout=0.2, noise_std=0.05, aggr='mean'):
        super().__init__()
        self.embed = nn.Embedding(num_nodes, dim_embedding)
        self.conv1 = SAGEConv(dim_embedding, hidden_channels, aggr=aggr)
        self.conv2 = SAGEConv(hidden_channels, num_classes, aggr=aggr)
        self.dropout = nn.Dropout(p=dropout)
        self.noise_std = noise_std

    def forward(self, data):
        x = self.embed(torch.arange(data.num_nodes, device=data.edge_index.device))
        if self.training and self.noise_std > 0:
            x = x + torch.randn_like(x) * self.noise_std

        x = self.conv1(x, data.edge_index)
        x = F.relu(x)
        x = self.dropout(x)
        x = self.conv2(x, data.edge_index)
        return F.softmax(x, dim=1)


# ---------- Main Training ----------
if __name__ == "__main__":
    # File and training settings
    file_path = "queen5_5.col"  # Replace with your file
    base_lr = 0.0001
    dim_embedding = 64
    hidden_channels = 64
    num_classes = 7 # equal to colors
    dropout=0.1
    noise_std=0.05
    num_epochs = 30000
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    # Load data
    edge_index, num_nodes = load_col_file(file_path)
    visualize_graph(edge_index, num_nodes)
    data = Data(edge_index=edge_index, num_nodes=num_nodes).to(device)

    # Model and optimizer
    model = SimpleSAGE(num_nodes, dim_embedding, hidden_channels, num_classes,dropout,noise_std).to(device)
    optimizer = torch.optim.Adam(model.parameters(), lr=base_lr,weight_decay=1e-2)
    scheduler = LambdaLR(optimizer, lr_lambda=cyclical_temperature_schedule(base_lr,warmup_epochs = 100,
                                                                            cold_factor=1,
                                                                            hot_factor=10,
                                                                            cycle_length=100,
                                                                            hot_phase_length=10))

    best_coloring = None
    best_cost = float("inf")
    conflict_log = []
    lr_log = []

    for epoch in range(num_epochs):
        model.train()
        optimizer.zero_grad()
        probs = model(data)
        loss = loss_func_pyg(probs, data.edge_index)
        loss.backward()
        optimizer.step()
        scheduler.step()

        model.eval()
        with torch.no_grad():
            preds = probs.argmax(dim=1)
            conflict = loss_func_color_hard_pyg(preds, data.edge_index)
            if conflict.item() == 0:
              lr_log.append(optimizer.param_groups[0]['lr'])
              conflict_log.append(conflict.item())
              best_cost = conflict.item()
              best_coloring = preds.clone()
              print("Colored")
              break

            if conflict.item() < best_cost:
                best_cost = conflict.item()
                best_coloring = preds.clone()
            lr_log.append(optimizer.param_groups[0]['lr'])
            conflict_log.append(conflict.item())

        if epoch % 100 == 0:
            print(f"Epoch {epoch:02d} | Potts Loss: {loss.item():.4f} | Conflicts: {conflict.item()} | Best: {best_cost} | lr: {optimizer.param_groups[0]['lr']}")

    # Visualize best coloring after training
    plot_node_predictions(data.edge_index, F.one_hot(best_coloring, num_classes).float())

for p in model.parameters():
  print(p.shape)

def copy_node_embeddings(source_model, target_model):
    """
    Copies the learned node embeddings from one model to another.

    :param source_model: Model with trained nn.Embedding
    :param target_model: Model to receive the embedding weights
    """
    if not hasattr(source_model, 'embed') or not hasattr(target_model, 'embed'):
        raise ValueError("Both models must have an 'embed' attribute (nn.Embedding layer).")

    if source_model.embed.weight.shape != target_model.embed.weight.shape:
        raise ValueError(f"Embedding shape mismatch: {source_model.embed.weight.shape} vs {target_model.embed.weight.shape}")

    target_model.embed.weight.data.copy_(source_model.embed.weight.data)